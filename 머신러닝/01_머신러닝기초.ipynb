{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 머신러닝분류\n",
    "#### [1] 지도학습(Supervised Learning) : 답이 주어진 상태에서 학습\n",
    "* 회귀(Regression)\n",
    "* 분류(Classification)\n",
    "#### [2] 비지도학습(Unsupervised Learning) : 답을 모르고 학습\n",
    "* 군집화(Clustering)\n",
    "* 차원 축소(Dimension Reduction) : PCA(주성분 분석, Pricipal Component Analysis)\n",
    "#### [3] 강화 학습(Reinforcement Learning) : 답을 모르고 있는 상태에서 답을 알아가는 강한 인공지능(자아를 갖음, 인간수준)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 퍼셉트론과 XOR Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "def AND(x1,x2):\n",
    "    w1,w2,theta = 0.5,0.5,0.7\n",
    "    tmp = w1*x1 + w2*x2\n",
    "    if tmp <= theta : # 임계값\n",
    "        return 0\n",
    "    elif tmp > theta :\n",
    "        return 1\n",
    "    \n",
    "print(AND(0,0))\n",
    "print(AND(0,1))\n",
    "print(AND(1,0))\n",
    "print(AND(1,1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "def NAND(x1,x2):\n",
    "    w1,w2,theta = 0.5,0.5,0.7\n",
    "    tmp = w1*x1 + w2*x2\n",
    "    if tmp <= theta : # 임계값\n",
    "        return 1\n",
    "    elif tmp > theta :\n",
    "        return 0\n",
    "    \n",
    "print(NAND(0,0))\n",
    "print(NAND(0,1))\n",
    "print(NAND(1,0))\n",
    "print(NAND(1,1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "def OR(x1,x2):\n",
    "    w1,w2,theta = 0.5,0.5,0.4\n",
    "    tmp = w1*x1 + w2*x2\n",
    "    if tmp <= theta : # 임계값\n",
    "        return 0\n",
    "    elif tmp > theta :\n",
    "        return 1\n",
    "    \n",
    "print(OR(0,0))\n",
    "print(OR(0,1))\n",
    "print(OR(1,0))\n",
    "print(OR(1,1))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [3] Backpropagation : 1986년 제프리 힌튼(Geoffrey Hinton)\n",
    "샘플에 대한 신경망의 오차를 다시 출력층에서부터 입력층으로 거꾸로 전파시켜 각 층의 가중치(weight)를 계산하는 방법. 이를 통해 weight와 bias를 알맞게 학습할 수 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다층퍼셉트론으로 XOR Problem 해결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# XOR Problem : 서로 다른 weight을 갖는 다층신경망을 사용하여 해결\n",
    "def XOR(x1,x2):\n",
    "    s1 = NAND(x1,x2)\n",
    "    s2 = OR(x1,x2)\n",
    "    y = AND(s1,s2)\n",
    "    return y\n",
    "\n",
    "print(XOR(0,0))\n",
    "print(XOR(0,1))\n",
    "print(XOR(1,0))\n",
    "print(XOR(1,1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 회귀(Regression)모델\n",
    "####  [1] 선형 회귀(Linear Regression) : 1차 함수, 직선의 방정식\n",
    "#### [2] 가중치(weight) : 입력변수가 출력에 영향을 미치는 정도를 설정, 기울기 값, 회귀계수\n",
    "#### [3] 편향(Bias) : 기본 출력 값이 활성화 되는 정도를 설정, y절편, 회귀 계수\n",
    "#### [4] 비용함수(Cost Function) : 2차 함수, 포물선의 방정식, (예측값 - 실제값)^2\n",
    "* cost(비용) = 오차 = 에러 = 손실(loss)\n",
    "* cost(W,b) = (H(x) - y)^2 \n",
    "\n",
    "#### [5] 예측(가설,Hypothesis)함수 : predict, H(x) : 예측값, y값 : 답,결정값,target,label,입력,피쳐(feature) \n",
    "* H(X) = W*X + b\n",
    "\n",
    "#### [6] 경사 하강법(Gredient Descent Algorithm)\n",
    "#### : 비용 이 작은 Weight(가중치) 값을 구하는 알고리즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:-1, cost:  18.666666666666668\n",
      "w: 0, cost:  4.666666666666667\n",
      "w: 1, cost:  0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 비용 함수의 구현\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def cost(x,y,w) : \n",
    "    c = 0\n",
    "    for k in range(len(x)):\n",
    "        hx = w * x[k]           # 예측 함수(방정식)\n",
    "        loss = (hx - y[k])**2   # (예측값 - 실제값)^2\n",
    "        c += loss\n",
    "    return c/len(x)             # 평균 제곱 오차\n",
    "\n",
    "x = [1,2,3]\n",
    "y = [1,2,3]\n",
    "\n",
    "print('w:-1, cost: ',cost(x,y,-1))   # hx = [-1,-2,-3] ,cost : 18.666666666666668\n",
    "print('w: 0, cost: ',cost(x,y,0))\n",
    "print('w: 1, cost: ',cost(x,y,1))    # w=1 ,cost = 0.0 ,최저점\n",
    "\n",
    "# 비용함수의 시각화 : x축은 가중치, y축은 cost\n",
    "for k in range(-30,50):\n",
    "    w = k/10\n",
    "    c = cost(x,y,w)\n",
    "    plt.plot(w,c,'ro')\n",
    "\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 미분 : 순간 변화량, 기울기, x축으로 1만큼 움직였을 때 y축으로 움직인 거리\n",
    "#### 함수의 미분 공식 정리 : f(x) = x^n ===> f'(x)= n*x^(n-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------ start learning!!\n",
      "[000] cost: 378.0 old 100 weight 5.8\n",
      "[001] cost: 107.51999999999998 old 378.0 weight 3.56\n",
      "[002] cost: 30.583466666666666 old 107.51999999999998 weight 2.365333333333333\n",
      "[003] cost: 8.69929718518518 old 30.583466666666666 weight 1.7281777777777778\n",
      "[004] cost: 2.47446675489712 old 8.69929718518518 weight 1.3883614814814815\n",
      "[005] cost: 0.7038483213929583 old 2.47446675489712 weight 1.2071261234567903\n",
      "[006] cost: 0.2002057447517751 old 0.7038483213929583 weight 1.1104672658436214\n",
      "[007] cost: 0.05694741184050483 old 0.2002057447517751 weight 1.0589158751165981\n",
      "[008] cost: 0.016198374923521403 old 0.05694741184050483 weight 1.0314218000621858\n",
      "[009] cost: 0.004607537756023892 old 0.016198374923521403 weight 1.0167582933664991\n",
      "[010] cost: 0.0013105885172690224 old 0.004607537756023892 weight 1.008937756462133\n",
      "[011] cost: 0.0003727896226898598 old 0.0013105885172690224 weight 1.004766803446471\n",
      "[012] cost: 0.0001060379371206724 old 0.0003727896226898598 weight 1.002542295171451\n",
      "[013] cost: 3.0161902114324568e-05 old 0.0001060379371206724 weight 1.0013558907581073\n",
      "[014] cost: 8.579385490296031e-06 old 3.0161902114324568e-05 weight 1.0007231417376572\n",
      "[015] cost: 2.4403585394623746e-06 old 8.579385490296031e-06 weight 1.000385675593417\n",
      "[016] cost: 6.941464290019953e-07 old 2.4403585394623746e-06 weight 1.0002056936498225\n",
      "[017] cost: 1.9744609536079287e-07 old 6.941464290019953e-07 weight 1.0001097032799053\n",
      "[018] cost: 5.616244490256599e-08 old 1.9744609536079287e-07 weight 1.0000585084159495\n",
      "[019] cost: 1.597509543893363e-08 old 5.616244490256599e-08 weight 1.0000312044885065\n",
      "[020] cost: 4.544027147100321e-09 old 1.597509543893363e-08 weight 1.0000166423938701\n",
      "[021] cost: 1.292523277397425e-09 old 4.544027147100321e-09 weight 1.0000088759433974\n",
      "[022] cost: 3.6765106557353845e-10 old 1.292523277397425e-09 weight 1.0000047338364786\n",
      "[023] cost: 1.0457630309680019e-10 old 3.6765106557353845e-10 weight 1.0000025247127886\n",
      "[024] cost: 2.974614843652282e-11 old 1.0457630309680019e-10 weight 1.0000013465134872\n",
      "[025] cost: 8.461126664488492e-12 old 2.974614843652282e-11 weight 1.0000007181405266\n",
      "[026] cost: 2.4067204744183466e-12 old 8.461126664488492e-12 weight 1.0000003830082809\n",
      "[027] cost: 6.845782681618235e-13 old 2.4067204744183466e-12 weight 1.0000002042710832\n",
      "[028] cost: 1.9472448527085553e-13 old 6.845782681618235e-13 weight 1.0000001089445776\n",
      "[029] cost: 5.538829787885448e-14 old 1.9472448527085553e-13 weight 1.0000000581037747\n",
      "[030] cost: 1.575489361658054e-14 old 5.538829787885448e-14 weight 1.0000000309886798\n",
      "[031] cost: 4.481391952569267e-15 old 1.575489361658054e-14 weight 1.000000016527296\n",
      "[032] cost: 1.2747070573345361e-15 old 4.481391952569267e-15 weight 1.0000000088145578\n",
      "[033] cost: 3.6258332831368247e-16 old 1.2747070573345361e-15 weight 1.0000000047010975\n",
      "[034] cost: 1.0313481853667948e-16 old 3.6258332831368247e-16 weight 1.000000002507252\n",
      "[035] cost: 2.933612341505031e-17 old 1.0313481853667948e-16 weight 1.000000001337201\n",
      "[036] cost: 8.344497274162171e-18 old 2.933612341505031e-17 weight 1.000000000713174\n",
      "[037] cost: 2.3735464825145457e-18 old 8.344497274162171e-18 weight 1.0000000003803595\n",
      "[038] cost: 6.751422044227725e-19 old 2.3735464825145457e-18 weight 1.0000000002028584\n",
      "[039] cost: 1.920405593646558e-19 old 6.751422044227725e-19 weight 1.0000000001081912\n",
      "[040] cost: 5.462493428133877e-20 old 1.920405593646558e-19 weight 1.000000000057702\n",
      "[041] cost: 1.5537773320496455e-20 old 5.462493428133877e-20 weight 1.0000000000307745\n",
      "[042] cost: 4.419657596231936e-21 old 1.5537773320496455e-20 weight 1.000000000016413\n",
      "[043] cost: 1.2571515848923574e-21 old 4.419657596231936e-21 weight 1.0000000000087537\n",
      "[044] cost: 3.5758710617043957e-22 old 1.2571515848923574e-21 weight 1.0000000000046687\n",
      "[045] cost: 1.0171864166944069e-22 old 3.5758710617043957e-22 weight 1.00000000000249\n",
      "[046] cost: 2.8933990559698716e-23 old 1.0171864166944069e-22 weight 1.000000000001328\n",
      "[047] cost: 8.231253103671774e-24 old 2.8933990559698716e-23 weight 1.0000000000007083\n",
      "[048] cost: 2.341362175139032e-24 old 8.231253103671774e-24 weight 1.0000000000003777\n",
      "[049] cost: 6.658942698258942e-25 old 2.341362175139032e-24 weight 1.0000000000002014\n",
      "[050] cost: 1.891893061517278e-25 old 6.658942698258942e-25 weight 1.0000000000001075\n",
      "[051] cost: 5.389865839559056e-26 old 1.891893061517278e-25 weight 1.0000000000000573\n",
      "[052] cost: 1.5315340044413334e-26 old 5.389865839559056e-26 weight 1.0000000000000306\n",
      "[053] cost: 4.38172789805011e-27 old 1.5315340044413334e-26 weight 1.0000000000000164\n",
      "[054] cost: 1.2599423424554927e-27 old 4.38172789805011e-27 weight 1.0000000000000087\n",
      "[055] cost: 3.46129156767911e-28 old 1.2599423424554927e-27 weight 1.0000000000000047\n",
      "[056] cost: 1.0355442841244991e-28 old 3.46129156767911e-28 weight 1.0000000000000024\n",
      "[057] cost: 2.677196697093809e-29 old 1.0355442841244991e-28 weight 1.0000000000000013\n",
      "[058] cost: 8.283039504820624e-30 old 2.677196697093809e-29 weight 1.0000000000000007\n",
      "[059] cost: 1.791371638939381e-30 old 8.283039504820624e-30 weight 1.0000000000000004\n",
      "[060] cost: 9.203377227578472e-31 old 1.791371638939381e-30 weight 1.0000000000000002\n",
      "[061] cost: 3.4512664603419266e-31 old 9.203377227578472e-31 weight 1.0\n",
      "[062] cost: 0.0 old 3.4512664603419266e-31 weight 1.0\n",
      "[063] cost: 0.0 old 0.0 weight 1.0\n",
      "------------------ end learning!!\n",
      "weight: 1.0 train: 64 회\n"
     ]
    }
   ],
   "source": [
    "# 경사 하강법 알고리즘 함수 구현, 미분 적용\n",
    "def gradient_descent(x,y,w) : \n",
    "    c = 0\n",
    "    for k in range(len(x)):\n",
    "        hx = w*x[k]\n",
    "        loss = (hx - y[k])*x[k]     # 곱하기 2를 생략한 비용함수의 미분\n",
    "        c += loss\n",
    "        # 비용함수의 미분 : cost(w) = (w*x[k] - y[k])^2 의 미분\n",
    "        # cost(w) = w^2 * x[k]^2 - 2*w*x[k]*y[k] + y[k]^2\n",
    "        # cost'(w) = 2*w*x[k]^2 - 2*x[k]*y[k]\n",
    "    return c/len(x)\n",
    "\n",
    "# 학습 시작(train,fit) 시작\n",
    "print('------------------ start learning!!')\n",
    "w, old = 10, 100\n",
    "for k in range(1000):\n",
    "    c = cost(x,y,w)\n",
    "    grad = gradient_descent(x,y,w)\n",
    "    w -= 0.1*grad   # 0.1 : 학습율(learning rate) , 하이퍼 파라메터, 가중치의 업데이트실행\n",
    "    print('[%03d]'%k,'cost:',c,'old',old,'weight',w)\n",
    "#     if c == 0.0:    # cost의 변화가 없을 때\n",
    "    if c >=old and abs(c - old) < 1.0e-15 :\n",
    "        break\n",
    "    old = c\n",
    "\n",
    "print('------------------ end learning!!')\n",
    "print('weight:',w,'train:',k+1,'회')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "# 알고리즘 구현 : 비용함수와 경사 하강법 알고리즘 함수 구현\n",
    "\n",
    "# (1) 비용함수 구현\n",
    "def cost(x,y,w) : \n",
    "    c = 0\n",
    "    for k in range(len(x)):\n",
    "        hx = w * x[k]           # 예측 함수(방정식)\n",
    "        loss = (hx - y[k])**2   # (예측값 - 실제값)^2\n",
    "        c += loss\n",
    "    return c/len(x)             # 평균 제곱 오차\n",
    "\n",
    "# (2) 경사하강법 알고리즘 함수 구현\n",
    "def gradient_descent(x,y,w) : \n",
    "    c = 0\n",
    "    for k in range(len(x)):\n",
    "        hx = w*x[k]\n",
    "        loss = (hx - y[k])*x[k]     # 곱하기 2를 생략한 비용함수의 미분\n",
    "        c += loss\n",
    "        # 비용함수의 미분 : cost(w) = (w*x[k] - y[k])^2 의 미분\n",
    "        # cost(w) = w^2 * x[k]^2 - 2*w*x[k]*y[k] + y[k]^2\n",
    "        # cost'(w) = 2*w*x[k]^2 - 2*x[k]*y[k]\n",
    "    return c/len(x)\n",
    "\n",
    "# (3) 학습(fit)함수 구현\n",
    "def fit(x,y):\n",
    "    print('------------------ start learning!!')\n",
    "    w, old = 10, 100\n",
    "    for k in range(1000):\n",
    "        c = cost(x,y,w)\n",
    "        grad = gradient_descent(x,y,w)\n",
    "        w -= 0.1*grad   # 0.1 : 학습율(learning rate) , 하이퍼 파라메터, 가중치의 업데이트실행\n",
    "        print('[%03d]'%k,'cost:',c,'old',old,'weight',w)\n",
    "        #     if c == 0.0:    # cost의 변화가 없을 때\n",
    "        if c >=old and abs(c - old) < 1.0e-15 :\n",
    "            break\n",
    "        old = c\n",
    "    print('------------------ end learning!!')\n",
    "    return w\n",
    "\n",
    "# (4) 예측(predict) 함수 구현\n",
    "def predict(x,w): \n",
    "    hx = w*np.array(x)\n",
    "    return list(hx)\n",
    "\n",
    "# (5) 정확도(평가지표) 측정 함수 구현 : 정확도 검증(Valiation)\n",
    "# <1> 분류(classification) 일 때 : 정확도(%)\n",
    "def get_accuaracy(x_test,y_test,w):\n",
    "    y_pred = predict(x_test,w)\n",
    "    print(y_pred)\n",
    "    correct = 0\n",
    "    for k,_ in enumerate(y_test) :\n",
    "        if y_test[k] == y_pred[k]:      # 실제값과 예측값이 같으면 correct를 1증가\n",
    "            correct += 1\n",
    "    accuracy = round(correct/len(y_test),2) # 맞은 갯수/전체 갯수\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# <2> 회귀(Linear Regression) 일 때 : RMSE(Root Mean Squared Error,평균 제곱근 오차)\n",
    "def get_rmse(x_test,y_test,w):\n",
    "    y_pred = predict(x_test,w)\n",
    "    print(y_pred)\n",
    "    squared_error = 0\n",
    "    for k,_ in enumerate(y_test):\n",
    "        squared_error += (y_pred[k] - y_test[k])**2 # 오차의 제곱을 합한다\n",
    "    mse = squared_error/len(y_test)   # 오차의 제곱의 평균 \n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------ start learning!!\n",
      "[000] cost: 704.0 old 100 weight 1.1999999999999993\n",
      "[001] cost: 7.040000000000012 old 704.0 weight 2.08\n",
      "[002] cost: 0.07040000000000013 old 7.040000000000012 weight 1.992\n",
      "[003] cost: 0.0007039999999999871 old 0.07040000000000013 weight 2.0008\n",
      "[004] cost: 7.03999999999845e-06 old 0.0007039999999999871 weight 1.99992\n",
      "[005] cost: 7.040000000016923e-08 old 7.03999999999845e-06 weight 2.0000080000000002\n",
      "[006] cost: 7.040000000546988e-10 old 7.040000000016923e-08 weight 1.9999992\n",
      "[007] cost: 7.040000000689097e-12 old 7.040000000546988e-10 weight 2.00000008\n",
      "[008] cost: 7.039999989746739e-14 old 7.040000000689097e-12 weight 1.999999992\n",
      "[009] cost: 7.039999978378055e-16 old 7.039999989746739e-14 weight 2.0000000008\n",
      "[010] cost: 7.040001164984472e-18 old 7.039999978378055e-16 weight 1.99999999992\n",
      "[011] cost: 7.040001164984472e-20 old 7.040001164984472e-18 weight 2.000000000008\n",
      "[012] cost: 7.039830636607046e-22 old 7.040001164984472e-20 weight 1.9999999999992\n",
      "[013] cost: 7.040612264052197e-24 old 7.039830636607046e-22 weight 2.00000000000008\n",
      "[014] cost: 7.028750665519215e-26 old 7.040612264052197e-24 weight 1.999999999999992\n",
      "[015] cost: 6.888333424389875e-28 old 7.028750665519215e-26 weight 2.000000000000001\n",
      "[016] cost: 7.257520328033308e-30 old 6.888333424389875e-28 weight 2.0\n",
      "[017] cost: 0.0 old 7.257520328033308e-30 weight 2.0\n",
      "[018] cost: 0.0 old 0.0 weight 2.0\n",
      "------------------ end learning!!\n",
      "weight: 2.0\n",
      "y_pred [12.0, 14.0, 16.0, 18.0, 20.0]\n",
      "[20.0, 40.0, 60.0]\n",
      "Accuracy: 0.67\n",
      "[20.0, 40.0, 60.0]\n",
      "RMSE: 5.773502691896258\n"
     ]
    }
   ],
   "source": [
    "# 머신 러닝 사용자가 구현할 부분 : 모델 구현\n",
    "\n",
    "# (1) fit() 함수를 호출하여 학습 수행\n",
    "x_train = [1,2,3,4,5]\n",
    "y_train = [2,4,6,8,10]\n",
    "\n",
    "w = fit(x_train,y_train)\n",
    "print('weight:',w)  \n",
    "\n",
    "# (2) predict() 함수를 호출하여 예측\n",
    "x_pred = [6,7,8,9,10]\n",
    "y_pred = predict(x_pred,w)\n",
    "print('y_pred',y_pred)\n",
    "\n",
    "# (3) 정확도 측정\n",
    "# 분류 모델인 경우\n",
    "x_test = [10,20,30]\n",
    "y_test = [20,40,70]\n",
    "\n",
    "accuracy = get_accuaracy(x_test,y_test,w)\n",
    "print('Accuracy:',accuracy)            # 절대지표\n",
    "\n",
    "# 회귀 모델인 경우\n",
    "x_test = [10,20,30]\n",
    "y_test = [20,40,70]\n",
    "\n",
    "rmse = get_rmse(x_test,y_test,w)\n",
    "print('RMSE:',rmse)                    # 상대지표"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
